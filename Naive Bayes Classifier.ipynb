{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a project I did while I was studying machine learning at the University of Melbourne in 2019. Yes, we can use scikit-learn to build a much handier and more comprehensive Naive Bayes classifier -  This was just an exercise with the purpose to <i> understand </i> what's behind Naive Bayes models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Clssifier - built from scratch\n",
    "#### This is a Naive Bayes classifier built without the help of any packages (except 'math'), even pandas.  It reads a csv file whose last column of each row indicats the class of the instance, trains the model and predicts the class of instances. It also outputs accuracy and error rate of the classifier and information gain of each attribute in the dataset. In the end, this script investigated the impact of missing values on a Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used the whole dataset as both training and testing dataset for all datasets in this project.\n",
    "# modify the following according to the dataset of interest\n",
    "f = open('hepatitis.csv','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def preprocess(file):\n",
    "    dataset=[]\n",
    "    for line in file.readlines():\n",
    "        dataset.append(line.strip().split(','))\n",
    "    return dataset\n",
    "\n",
    "dataset = preprocess(f)\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of dictionaries in the format of {attribute value : counts}\n",
    "def attribute_list(dataset):\n",
    "    '''for each attribute, count the number of instances that have a certain value for that attribtue'''\n",
    "    list_attr=[]\n",
    "    for x in range(len(dataset[0])-1):\n",
    "        list_attr.append({})\n",
    "\n",
    "    for i in dataset:\n",
    "        for c in range(len(i)-1):\n",
    "            if i[c] not in list_attr[c].keys():\n",
    "                list_attr[c][i[c]] = 1\n",
    "            else:\n",
    "                list_attr[c][i[c]] += 1\n",
    "    return list_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(dataset):\n",
    "    '''The posteriors model is a list of nested dictionaries, where each item in the list represents \n",
    "    the counts associated with one attribute.\n",
    "    The attributes are ordered as in the dataset.'''\n",
    "    model=[{} for i in range(len(dataset[0])-1)] # a list of dictionaries as framework for counting\n",
    "    # counting priors and posteriors\n",
    "    count_instances = 0\n",
    "    class_dict = {}\n",
    "\n",
    "    for instance in dataset:\n",
    "        # priors\n",
    "        count_instances+=1\n",
    "        if instance[-1] not in class_dict.keys():\n",
    "            class_dict[instance[-1]] = 1\n",
    "        else:\n",
    "            class_dict[instance[-1]] += 1\n",
    "        # posteriors\n",
    "        for index in range(len(instance)-1):\n",
    "            if instance[-1] not in model[index].keys():\n",
    "                model[index].update({instance[-1]:{}})\n",
    "            if instance[index] not in model[index][instance[-1]].keys():\n",
    "                model[index][instance[-1]].update({instance[index]:1})\n",
    "            else:                \n",
    "                model[index][instance[-1]][instance[index]]+=1\n",
    "    return {'count of instances': count_instances, 'priors': class_dict, 'posteriors': model}\n",
    "model = train(dataset)\n",
    "###### If an IndexError occurs, restart the file and re-run this function (because the dataset was trained more than once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count of instances': 155, 'priors': {'LIVE': 123, 'DIE': 32}, 'posteriors': [{'LIVE': {'female': 16, 'male': 107}, 'DIE': {'male': 32}}, {'LIVE': {'no': 56, 'yes': 66, '?': 1}, 'DIE': {'no': 20, 'yes': 12}}, {'LIVE': {'yes': 101, 'no': 22}, 'DIE': {'yes': 30, 'no': 2}}, {'LIVE': {'yes': 52, 'no': 70, '?': 1}, 'DIE': {'no': 30, 'yes': 2}}, {'LIVE': {'yes': 84, 'no': 38, '?': 1}, 'DIE': {'yes': 9, 'no': 23}}, {'LIVE': {'yes': 100, 'no': 22, '?': 1}, 'DIE': {'no': 10, 'yes': 22}}, {'LIVE': {'no': 22, 'yes': 96, '?': 5}, 'DIE': {'yes': 24, '?': 5, 'no': 3}}, {'LIVE': {'yes': 70, 'no': 47, '?': 6}, 'DIE': {'yes': 14, 'no': 13, '?': 5}}, {'LIVE': {'yes': 101, 'no': 18, '?': 4}, 'DIE': {'no': 12, 'yes': 19, '?': 1}}, {'LIVE': {'yes': 90, 'no': 29, '?': 4}, 'DIE': {'no': 22, 'yes': 9, '?': 1}}, {'LIVE': {'yes': 113, 'no': 6, '?': 4}, 'DIE': {'yes': 17, 'no': 14, '?': 1}}, {'LIVE': {'yes': 112, '?': 4, 'no': 7}, 'DIE': {'yes': 20, 'no': 11, '?': 1}}, {'LIVE': {'no': 78, 'yes': 45}, 'DIE': {'no': 7, 'yes': 25}}]}\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "def predict(model, instance):\n",
    "    '''The instance is supposed to have the same format as instances in the preprocessed dataset \n",
    "    (a list of attribute values, where the attributes are ordered as in the dataset.).'''\n",
    "    prob_classes={}\n",
    "\n",
    "    for elem in model['priors'].keys():\n",
    "        prob_classes.update({elem : (model['priors'][elem]/model['count of instances'])})\n",
    "\n",
    "    for index in range(len(instance)-1):\n",
    "        for key in prob_classes.keys():\n",
    "        # if there is no combination of attribute value and class of the instance, add that combination into the model with a count of 0\n",
    "            if instance[index] not in model['posteriors'][index][key].keys():\n",
    "                model['posteriors'][index][key].update({instance[index] : 0})\n",
    "\n",
    "    # create a list of dictionaries: {class : probability(attribute = value | class)}\n",
    "    prob_cond=[]\n",
    "    epsilon = 0.00001 # epsilon smoothing, this value may be edited\n",
    "    for index in range(len(instance)-1):\n",
    "        prob_cond.append({})\n",
    "        for key in prob_classes.keys():\n",
    "            # smoothing\n",
    "            conditional_prob = (model['posteriors'][index][key][instance[index]] / model['priors'][key])\n",
    "            if conditional_prob == 0:\n",
    "                conditional_prob = epsilon\n",
    "            prob_cond[index].update({ key : conditional_prob})\n",
    "\n",
    "    #calculating the probability that the instance belongs to each class\n",
    "    predict_class_prob = {key: 1 for key in prob_classes.keys()}\n",
    "    for index in range(len(prob_cond)-1):\n",
    "        for key in prob_classes.keys():\n",
    "            predict_class_prob[key] *= prob_cond[index][key]\n",
    "\n",
    "    for key in predict_class_prob.keys():\n",
    "        if predict_class_prob[key] == max(predict_class_prob.values()):\n",
    "            predicted_class = key\n",
    "    \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'LIVE']\n",
      "The predicted class of this instance is LIVE\n"
     ]
    }
   ],
   "source": [
    "# a demonstration of the predict() function\n",
    "instance = dataset[0]\n",
    "print(instance)\n",
    "print('The predicted class of this instance is', predict(model, instance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for this model and dataset is 0.8258\n",
      "The error rate for this model and dataset is 0.1742\n"
     ]
    }
   ],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate(model, dataset):\n",
    "    #create a nested list where each item is a pair of [actual class vs. predicted class] for each instance\n",
    "    predictions = []\n",
    "\n",
    "    for instance in dataset:\n",
    "        true_class = instance[-1]\n",
    "        predicted_class = predict(model, instance)\n",
    "        predictions.append([true_class,predicted_class])\n",
    "    #print(predictions)\n",
    "\n",
    "    # accuracy\n",
    "    count_true = 0\n",
    "    count_total = 0\n",
    "    for elem in predictions:\n",
    "        count_total +=1\n",
    "        if elem[0] == elem[1]:\n",
    "            count_true +=1\n",
    "    accuracy = count_true/count_total\n",
    "    error_rate = 1 - accuracy\n",
    "    \n",
    "    \n",
    "    print('The accuracy for this model and dataset is', round(accuracy,4))\n",
    "    print('The error rate for this model and dataset is', round(error_rate,4))\n",
    "\n",
    "evaluate(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(list_of_prob): \n",
    "    '''calculates entropy given a list of probabilities'''\n",
    "    e = 0\n",
    "    for elem in list_of_prob:\n",
    "        e += (elem * math.log(elem,2))\n",
    "    entropy = -e\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7346451526501956, [0.0366, 0.0153, 0.0145, 0.0865, 0.0832, 0.0138, 0.0258, 0.0199, 0.0355, 0.1061, 0.1283, 0.0768, 0.0849])\n"
     ]
    }
   ],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(model):\n",
    "    '''This function returns a list of infomation gain for each attribute. \n",
    "    The attributes are ordered as in the dataset.'''\n",
    "    # entropy at top level\n",
    "    list_of_count = model['priors'].values()\n",
    "    list_of_prob = []\n",
    "    for elem in list_of_count:\n",
    "        prob = elem / sum(list_of_count)\n",
    "        list_of_prob.append(prob)\n",
    "    top_level_entropy = entropy(list_of_prob)\n",
    "    #print(top_level_entropy)\n",
    "\n",
    "    #list of prob for one attribute value\n",
    "    #framework\n",
    "    attr_list_prob = []\n",
    "    for index in range(len(attribute_list(dataset))):\n",
    "        attr_list_prob.append({})\n",
    "        for key in attribute_list(dataset)[index].keys():\n",
    "            attr_list_prob[index].update({key:[]})\n",
    "\n",
    "    for index in range(len(model['posteriors'])):\n",
    "        for cls in model['posteriors'][index].keys():\n",
    "            for key in model['posteriors'][index][cls].keys():\n",
    "                # number of instance of one class that has this value            \n",
    "                num_ins = model['posteriors'][index][cls][key] \n",
    "                #total number of instances that have this attribtue value\n",
    "                ttl_ins = attribute_list(dataset)[index][key]\n",
    "                prob = num_ins / ttl_ins\n",
    "                list_of_prob = attr_list_prob[index][key]\n",
    "                list_of_prob.append(prob)\n",
    "    #print(attr_list_prob)\n",
    "\n",
    "    # calculating entropy for each attribute value\n",
    "    list_value_ent = []\n",
    "    for index in range(len(attr_list_prob)):\n",
    "        list_value_ent.append({})\n",
    "        for key in attr_list_prob[index].keys():\n",
    "            if 0 in attr_list_prob[index][key]:\n",
    "                entropy_of_value = 0\n",
    "            else:\n",
    "                entropy_of_value = entropy(attr_list_prob[index][key])\n",
    "            list_value_ent[index].update({key : entropy_of_value})\n",
    "    #print(list_value_ent)\n",
    "\n",
    "    list_attr_MI = []\n",
    "    list_attr_IG = []\n",
    "    for index in range(len(list_value_ent)):\n",
    "        #list_attr_MI.append({})\n",
    "        MI = 0\n",
    "        for key in list_value_ent[index].keys():\n",
    "            #total number of instances that have this attribtue value:\n",
    "            value_ins = attribute_list(dataset)[index][key]\n",
    "            total_ins = sum(attribute_list(dataset)[index].values())\n",
    "            info = (value_ins/total_ins) * list_value_ent[index][key]\n",
    "            MI += info\n",
    "            IG = round(top_level_entropy - MI, 4)\n",
    "        list_attr_MI.append(MI)\n",
    "        list_attr_IG.append(IG)\n",
    "    #print(list_attr_MI)\n",
    "    return top_level_entropy, list_attr_IG\n",
    "\n",
    "print(info_gain(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A run-through for a list of sample datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hepatitis.csv\n",
      "The accuracy for this model and dataset is 0.8258\n",
      "The error rate for this model and dataset is 0.1742\n",
      "the top level entropy for this dataset is 0.7346\n",
      "Information Gain for attributes: [0.0366, 0.0153, 0.0145, 0.0865, 0.0832, 0.0138, 0.0258, 0.0199, 0.0355, 0.1061, 0.1283, 0.0768, 0.0849]\n",
      "the arithmetic average of attribute IG's for this dataset: 0.0559\n",
      "**************************************************\n",
      "primary-tumor.csv\n",
      "The accuracy for this model and dataset is 0.5221\n",
      "The error rate for this model and dataset is 0.4779\n",
      "the top level entropy for this dataset is 3.6437\n",
      "Information Gain for attributes: [3.6437, 3.6437, 3.6437, 3.6437, 3.6437, 0.0584, 3.6437, 3.6437, 3.6437, 3.6437, 0.1774, 0.1914, 3.6437, 3.6437, 0.407, 0.8739, 3.6437]\n",
      "the arithmetic average of attribute IG's for this dataset: 2.6725\n",
      "**************************************************\n",
      "breast-cancer.csv\n",
      "The accuracy for this model and dataset is 0.7203\n",
      "The error rate for this model and dataset is 0.2797\n",
      "the top level entropy for this dataset is 0.8778\n",
      "Information Gain for attributes: [0.0106, 0.002, 0.0572, 0.069, 0.0534, 0.077, 0.0025, 0.0151, 0.0258]\n",
      "the arithmetic average of attribute IG's for this dataset: 0.0347\n",
      "**************************************************\n",
      "cmc.csv\n",
      "The accuracy for this model and dataset is 0.5051\n",
      "The error rate for this model and dataset is 0.4949\n",
      "the top level entropy for this dataset is 1.539\n",
      "Information Gain for attributes: [0.0709, 0.0401, 0.1113, 0.0098, 0.0026, 0.0305, 0.0325, 0.0158]\n",
      "the arithmetic average of attribute IG's for this dataset: 0.0392\n",
      "**************************************************\n",
      "car.csv\n",
      "The accuracy for this model and dataset is 0.6123\n",
      "The error rate for this model and dataset is 0.3877\n",
      "the top level entropy for this dataset is 1.2057\n",
      "Information Gain for attributes: [0.4618, 0.4819, 0.0045, 0.2197, 0.33, 0.6672]\n",
      "the arithmetic average of attribute IG's for this dataset: 0.3608\n",
      "**************************************************\n",
      "anneal.csv\n",
      "The accuracy for this model and dataset is 0.9889\n",
      "The error rate for this model and dataset is 0.0111\n",
      "the top level entropy for this dataset is 1.1898\n",
      "Information Gain for attributes: [1.1898, 0.0, 1.1898, 0.0513, 0.2965, 0.2861, 0.6467, 1.1898, 0.146, 0.2574, 0.0325, 1.1898, 0.0387, 0.0004, 0.0827, 0.1186, 0.206, 0.1369, 0.0, 0.1172, 0.0298, 0.0564, 0.0, 0.0156, 0.1372, 0.0, 0.0224, 0.0366, 0.0, 0.0, 0.0, 0.7105, 0.0659, 0.0473, 0.004]\n",
      "the arithmetic average of attribute IG's for this dataset: 0.2372\n",
      "**************************************************\n",
      "hypothyroid.csv\n",
      "The accuracy for this model and dataset is 0.356\n",
      "The error rate for this model and dataset is 0.644\n",
      "the top level entropy for this dataset is 0.2767\n",
      "Information Gain for attributes: [0.0004, 0.0009, 0.0012, 0.0001, 0.001, 0.0014, 0.0005, 0.0004, 0.0005, 0.0009, 0.0, 0.0001, 0.0094, 0.0041, 0.0058, 0.0058, 0.0057, 0.0026]\n",
      "the arithmetic average of attribute IG's for this dataset: 0.0023\n",
      "**************************************************\n",
      "mushroom.csv\n",
      "The accuracy for this model and dataset is 0.9926\n",
      "The error rate for this model and dataset is 0.0074\n",
      "the top level entropy for this dataset is 0.9991\n",
      "Information Gain for attributes: [0.0488, 0.0286, 0.036, 0.1924, 0.9061, 0.0142, 0.1009, 0.2302, 0.417, 0.0075, 0.1348, 0.2847, 0.2719, 0.2538, 0.2414, 0.0, 0.0238, 0.0385, 0.318, 0.4807, 0.202, 0.1568]\n",
      "the arithmetic average of attribute IG's for this dataset: 0.1995\n",
      "**************************************************\n",
      "nursery.csv\n",
      "The accuracy for this model and dataset is 0.4972\n",
      "The error rate for this model and dataset is 0.5028\n",
      "the top level entropy for this dataset is 1.7165\n",
      "Information Gain for attributes: [1.1511, 1.4023, 1.2787, 1.2745, 1.1261, 0.845, 0.5473, 1.7165]\n",
      "the arithmetic average of attribute IG's for this dataset: 1.1677\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "############################## printing results for all datasets ##############################\n",
    "datasets = ['hepatitis.csv', 'primary-tumor.csv', 'breast-cancer.csv', 'cmc.csv', 'car.csv', 'anneal.csv', 'hypothyroid.csv', 'mushroom.csv', 'nursery.csv']\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "    f = open(data,'r')\n",
    "    dataset = preprocess(f)\n",
    "    model = train(dataset)\n",
    "    evaluate(model, dataset)\n",
    "    top_level_entropy = info_gain(model)[0]\n",
    "    list_of_IG = info_gain(model)[1]\n",
    "    print('the top level entropy for this dataset is', round(top_level_entropy,4))\n",
    "    print('Information Gain for attributes:', list_of_IG)\n",
    "    avg = round(sum(list_of_IG)/len(list_of_IG), 4)\n",
    "    print('the arithmetic average of attribute IG\\'s for this dataset:', avg)\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation around Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new datasets for instances with and without missing values\n",
    "def missing_sets(dataset):\n",
    "    missing_set = []\n",
    "    no_miss_set = []\n",
    "    for instance in dataset:\n",
    "        for value in instance:\n",
    "            if value == '?':\n",
    "                missing_set.append(instance)\n",
    "        if '?' not in instance:\n",
    "            no_miss_set.append(instance)\n",
    "    return missing_set, no_miss_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance of data with MISSING VALUES:\n",
      "hepatitis.csv\n",
      "The accuracy for this model and dataset is 1.0\n",
      "The error rate for this model and dataset is 0.0\n",
      "**************************************************\n",
      "primary-tumor.csv\n",
      "The accuracy for this model and dataset is 0.5867\n",
      "The error rate for this model and dataset is 0.4133\n",
      "**************************************************\n",
      "breast-cancer.csv\n",
      "The accuracy for this model and dataset is 1.0\n",
      "The error rate for this model and dataset is 0.0\n",
      "**************************************************\n",
      "hypothyroid.csv\n",
      "The accuracy for this model and dataset is 0.3288\n",
      "The error rate for this model and dataset is 0.6712\n",
      "**************************************************\n",
      "mushroom.csv\n",
      "The accuracy for this model and dataset is 1.0\n",
      "The error rate for this model and dataset is 0.0\n",
      "**************************************************\n",
      "-----------------------------------------------------------------\n",
      "performance of data with NO MISSING VALUES:\n",
      "hepatitis.csv\n",
      "The accuracy for this model and dataset is 0.831\n",
      "The error rate for this model and dataset is 0.169\n",
      "**************************************************\n",
      "primary-tumor.csv\n",
      "The accuracy for this model and dataset is 0.7045\n",
      "The error rate for this model and dataset is 0.2955\n",
      "**************************************************\n",
      "breast-cancer.csv\n",
      "The accuracy for this model and dataset is 0.7329\n",
      "The error rate for this model and dataset is 0.2671\n",
      "**************************************************\n",
      "hypothyroid.csv\n",
      "The accuracy for this model and dataset is 0.3573\n",
      "The error rate for this model and dataset is 0.6427\n",
      "**************************************************\n",
      "mushroom.csv\n",
      "The accuracy for this model and dataset is 0.997\n",
      "The error rate for this model and dataset is 0.003\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# datasets with missing values\n",
    "datasets = ['hepatitis.csv', 'primary-tumor.csv', 'breast-cancer.csv', 'hypothyroid.csv', 'mushroom.csv']\n",
    "\n",
    "print('performance of data with MISSING VALUES:')\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "    f = open(data,'r')\n",
    "    dataset = preprocess(f)\n",
    "    ds = missing_sets(dataset)[0]\n",
    "    model = train(ds)\n",
    "    evaluate(model, ds)\n",
    "    print('*'*50)\n",
    "\n",
    "print('-'*65)\n",
    "print('performance of data with NO MISSING VALUES:')\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "    f = open(data,'r')\n",
    "    dataset = preprocess(f)\n",
    "    ds = missing_sets(dataset)[1]\n",
    "    model = train(ds)\n",
    "    evaluate(model, ds)\n",
    "    print('*'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of modes\n",
    "def mode_list(dataset):\n",
    "    mode_list=[]\n",
    "    list_attr = attribute_list(dataset)\n",
    "    for dicts in list_attr:\n",
    "        mode = list(dicts.keys())[0]\n",
    "        for key in dicts.keys():\n",
    "            if dicts[key] > dicts[mode] and key != '?': #### delete \" and key != '?' \" if keeping missing values\n",
    "                mode = key\n",
    "        mode_list.append(mode)\n",
    "    return mode_list\n",
    "\n",
    "# impute missing values by the attibute average (i.e. mode, in the case of categorical data)\n",
    "def mean_imputation(dataset):\n",
    "    for i in dataset:\n",
    "        for n in range(len(i)-1):\n",
    "            if i[n] == '?':\n",
    "                i[n] = mode_list(dataset)[n]\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new performance of dataset with missing values after mean imputation\n",
      "hepatitis.csv\n",
      "The accuracy for this model and dataset is 0.8\n",
      "The error rate for this model and dataset is 0.2\n",
      "**************************************************\n",
      "primary-tumor.csv\n",
      "The accuracy for this model and dataset is 0.5156\n",
      "The error rate for this model and dataset is 0.4844\n",
      "**************************************************\n",
      "breast-cancer.csv\n",
      "The accuracy for this model and dataset is 1.0\n",
      "The error rate for this model and dataset is 0.0\n",
      "**************************************************\n",
      "hypothyroid.csv\n",
      "The accuracy for this model and dataset is 0.3288\n",
      "The error rate for this model and dataset is 0.6712\n",
      "**************************************************\n",
      "mushroom.csv\n"
     ]
    }
   ],
   "source": [
    "print('new performance of dataset with missing values after mean imputation')\n",
    "\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "    f = open(data,'r')\n",
    "    dataset = preprocess(f)\n",
    "    imputed_dataset = mean_imputation(missing_sets(dataset)[0])\n",
    "    model = train(imputed_dataset)\n",
    "    evaluate(model, imputed_dataset)\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering a few Questions\n",
    "\n",
    "These question are answered on the basis of epsilon smoothing with epsilon = 0.00001.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "\n",
    "\n",
    "The accuracy metric of the classifier varies greatly with respect to the dataset, for example: the accuracy of the ‘mushroom’ dataset is as high as 0.9926 and 0.9889 for the ‘anneal’ dataset, whereas that for the hypothyroid dataset is only 0.356. Such variation can be partly explained by the information gain criterion. For datasets ‘mushroom’, ‘anneal’, ‘hepatitis’, ‘breast cancer’, ‘cmc’ and ‘hypothyroid’ (majority of the datasets), the accuracy generally decreases as the IG (arithmetically averaged from all attributes) decreases. However, the datasets ‘car’, ‘primary tumor’, ‘nursery’, and potentially ‘anneal’ are exceptions to this rule. One possible explanation is that these datasets have relatively high top-level entropy for the classes (however, this may not apply for ‘cmc’,) than other datasets. By observation, the measure MI does not seem to vary greatly among datasets, as IG = top entropy – MI, this higher top-level entropy would induce a higher IG for attributes. \n",
    "\n",
    "Also, these four datasets have both relatively large number of classes and attributes (both at least 4), which explains the high entropy values. This may be the reason why ‘cmc’ has high top-level entropy but an IG that is proportional to accuracy (because of only 3 classes). Also, although the dataset ‘mushroom’ has 22 classes, it has only 2 attributes, so the generalisation (dataset with both relatively large number of classes and attributes will have a higher top-level entropy and an IG that is not proportional to accuracy) could still hold.\n",
    "\n",
    "In addition, special attention may be paid to the datasets ‘primary tumor’ and ‘anneal’, where the actual number of classes is more than what is indicated in the dataset. This could contribute to the high entropy since the model would be less predicative in the case of “missing classes”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "For datasets ‘primary tumor’, ‘hepatitis’, and ‘breast cancer’, the accuracies of predictions on instances with and without missing values seem to differ, although the degree of difference is arguably acceptable. This could be the evidence that Naive Bayes performs ‘elegantly’ in the presence of missing values. For example, for ‘primary tumor’, the accuracy of predictions on data with missing values is 0.5687, while that for data where all values are present is 0.7045. This should be because that these datasets have small sample sizes. On contrary, for the datasets ‘mushroom’ and ‘hypothyroid’, the difference of accuracies between two sets of data is rather small (e.g. for ‘mushroom’, 0.997 vs. 1), since they have relatively large sample sizes.\n",
    "\n",
    "By observation, the number of values missing does not seem to matter. For the three datasets mentioned above, they happen to have large, moderate and small number of values missing, i.e. three different degrees of values missing. Still, they all differ in the performance of prediction on data with or without missing values.\n",
    "\n",
    "Counterintuitively, an imputation strategy (here I used attribute mode imputation) does not seem to improve the performances, and sometimes worsens them. This can be observed from datasets ‘primary tumor’ and ‘hepatitis’(from 0.5687 to 0.5156 and from 1 to 0.8). Both of the datasets have small sample sizes and a relatively large number of attributes. For other datasets, the accuracy after imputation stays the same as before imputation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
